{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8db110b",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6bedeb",
   "metadata": {},
   "source": [
    "We've poked around some textual archives data, and now let's look at preparing textual data to perform some natural language processing on it. \n",
    "\n",
    "First, we're going to need to import a bunch of libraries to get ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af476730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.downloader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55fdca",
   "metadata": {},
   "source": [
    "And now we need to use one of those libraries we imported to download some content that the rest of these libraries will need to function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ba8ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nms-\n",
      "[nltk_data]     workshop/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /Users/nms-\n",
      "[nltk_data]     workshop/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nms-\n",
      "[nltk_data]     workshop/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nms-\n",
      "[nltk_data]     workshop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nms-workshop/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96228450",
   "metadata": {},
   "source": [
    "And finally, we need to have some text to be working on here! Let's use one of the .ocr files in the `sample-documents` directory of the no-more-silence-sample dataset we're working with in this workshop. These files are text files (if you open one in a text editor you'll be able to see it) so they'll open easily in our progam and give us text to start working with. \n",
    "\n",
    "`ucsf_mss95-04_001_033.ocr` looks like a good one, let's open it up and read it into a variable with the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ca7932",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'no-more-silence-sample-main/sample-documents/ucsf_mss95-04_001_033.ocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno-more-silence-sample-main/sample-documents/ucsf_mss95-04_001_033.ocr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     sample_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'no-more-silence-sample-main/sample-documents/ucsf_mss95-04_001_033.ocr'"
     ]
    }
   ],
   "source": [
    "with open('no-more-silence-sample-main/sample-documents/ucsf_mss95-04_001_033.ocr', 'r') as file:\n",
    "    sample_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20932f34",
   "metadata": {},
   "source": [
    "Now we have the text from this file stored in a variable -- `sample_text` -- that we'll prepare for some NLP analysis using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8809fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4264d49",
   "metadata": {},
   "source": [
    "Right from the start we can see this is pretty messy. To do NLP we have to make things more uniform, so let's do that work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d229d7c",
   "metadata": {},
   "source": [
    "Firstly, let's turn this into a series of word we can work individually on. This is called \"tokenizing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1529052",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548ebf4",
   "metadata": {},
   "source": [
    "We can see we've got a lot of wild stuff going on in here. Let's start by removing non-alphanumeric characters to see if that gets rid of some of the wildness at the front (the poorly OCR'd hand-written text). (and we should note all the important information this is removed from this text by getting rid of this part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39e681",
   "metadata": {},
   "source": [
    "We'll do this using regular expressions, and the `re` library which handles this in python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf5b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88739b",
   "metadata": {},
   "source": [
    "This code will go through our `tokenized_text` list and replace any non-alphanumeric characters -- the `r'\\W+'` expression -- with nothing, thereby removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_no_punct = []\n",
    "for w in tokenized_text:\n",
    "    tokenized_text_no_punct.append(re.sub(r'\\W+', '', w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0324a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_text_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732381d",
   "metadata": {},
   "source": [
    "We might also want to remove 'stopwords', those words that help for a sentence but that have \"little meaning\" (lol) in and of themselves. These are words like 'the', 'a', 'an', 'by', etc. `nltk` contains a pre-set list of stopwords that we can use for this process, one of the things we downloaded earlier on in this workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4df9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd82afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b04a98",
   "metadata": {},
   "source": [
    "Now let's check each of the words in our list against the stopwords list, and leave them out if they're stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_no_stop = []\n",
    "for word in tokenized_text_no_punct:\n",
    "    if word not in stop:\n",
    "        tokenized_text_no_stop.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_text_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234b417",
   "metadata": {},
   "source": [
    "This is better, but there's still a lot of stuff we probably can't work with in there, for one thing all the totally empty tokens (`''`), so let's rewrite this so that it goes through and removes those too, and turns all these to lowercase while we're at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9685d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_no_stop = []\n",
    "for word in tokenized_text_no_punct:\n",
    "    if (word.lower() not in stop) and (word != ''):\n",
    "        tokenized_text_no_stop.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_text_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab46c47",
   "metadata": {},
   "source": [
    "This is looking a lot more uniform now. In a bit we'll see what else we can do to prepare it further. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a92dba",
   "metadata": {},
   "source": [
    "Now however, we can also start to look at some interesting functionality provided by `nltk`. Let's get it to label for us the parts of speech, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db18e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos = pos_tag(tokenized_text_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbd64e",
   "metadata": {},
   "source": [
    "Each word is now labelled with the part of speech it represents. These can be a bit tough to understand, so `nltk` does contain a built in key: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fd1ac",
   "metadata": {},
   "source": [
    "It's a good thing that we took a look at this help, because we can see that our order was a little off here. We should have labeled the parts of speech prior to making everything lowercase, because now proper nouns are not recognized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe0153",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefa2eb",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing words to their root form, whether or not the root form is a word itself. Again, `nltk` contains several built in function to do this work, each using slightly different approaches.\n",
    "\n",
    "We'll use the function `PorterStemmer()` for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58026546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c1e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "porter_stemmed = [porter.stem(t) for t in tokenized_text_no_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d32c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(porter_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7c67f",
   "metadata": {},
   "source": [
    "We can also take the approach of lemmatising, which is the process of reducing words to their root form only if the root form is also a word in the english language. For this, `nltk` comes with the `WordNetLemmatizer()`. (note that we don't have to import this library because we already imported `wordnet` at the very beginning of this workbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(t) for t in tokenized_text_no_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed64c2a",
   "metadata": {},
   "source": [
    "Now we've got some more-or-less cleaned text, and we can begin to do some NLP in the next half of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba862df2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
